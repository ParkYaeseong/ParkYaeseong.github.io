---
title: XGBoost
date: 2025-03-05 05:11:00 +09:00
categories: [Python]
tags: [Python, XGBoost]
---

XGBoost(Extreme Gradient Boosting)는 뛰어난 성능과 효율성으로 널리 사용되는 머신러닝 알고리즘입니다. 이 글에서는 XGBoost의 핵심 개념, 작동 원리, 과적합 방지 기법, 그리고 실제 사용 예제까지 자세히 살펴보겠습니다.

## 1. XGBoost란 무엇인가?

XGBoost는 **앙상블 학습(Ensemble Learning)** 방법 중 하나인 **그래디언트 부스팅(Gradient Boosting)** 을 기반으로 합니다. 앙상블 학습은 여러 개의 약한 학습기(Weak Learner)를 결합하여 강력한 학습기(Strong Learner)를 만드는 방법입니다.

### 1.1 앙상블 학습 (Ensemble Learning)

앙상블 학습은 여러 모델을 결합하여 더 나은 예측 성능을 얻는 기법입니다.

*   **Bootstrapping**: 원본 데이터에서 랜덤하게 샘플링하여 여러 개의 부분집합(subset)을 만듭니다.
*   **Bagging (Bootstrap Aggregating)**: 부트스트래핑으로 만든 데이터셋으로 여러 모델을 학습시키고, 그 결과(평균 또는 투표)를 결합합니다. (예: Random Forest)
*   **Boosting**: 이전 모델의 오차를 보완하는 방식으로 순차적으로 모델을 학습시킵니다. (예: AdaBoost, Gradient Boosting)
*   **Voting**: 여러 모델의 예측 결과를 투표(다수결 또는 가중 투표)를 통해 최종 예측을 결정합니다.
*   **Stacking**: 여러 모델의 예측 결과를 새로운 입력 데이터로 사용하여 최종 모델(Meta-Learner)을 학습시킵니다.
*   **Blending**: Stacking과 유사하지만, 교차 검증(Cross-Validation)을 사용하지 않고 홀드아웃(Hold-out) 검증 세트를 사용합니다.

### 1.2 Gradient Boosting

Gradient Boosting은 이전 모델이 예측하지 못한 **잔차(Residual)** 를 다음 모델이 학습하는 방식으로 작동합니다.  즉, 이전 모델의 오차를 새로운 목표값으로 설정하여 학습을 반복합니다.

*   **약한 학습기 (Weak Learner)**: 일반적으로 얕은 깊이의 결정 트리(Decision Tree)를 사용합니다.
*   **강한 학습기 (Strong Learner)**: 약한 학습기들을 순차적으로 결합하여 만들어집니다.
*   **잔차(Residual)**를 예측하도록 학습하는 과정을 반복하면서 점진적으로 모델이 데이터의 복잡한 패턴을 학습하게 됩니다.

### 1.3 XGBoost의 특징

XGBoost는 Gradient Boosting을 개선하여 다음과 같은 특징을 가집니다.

*   **속도 및 효율성**: 병렬 처리, 하드웨어 최적화, 효율적인 메모리 사용으로 대규모 데이터 처리에 용이합니다.
*   **정규화(Regularization)**: L1 (Lasso), L2 (Ridge) 정규화를 지원하여 과적합(Overfitting)을 방지합니다.
*   **결측값 처리**: 결측값이 있는 데이터를 자동으로 처리합니다.
*   **Tree Pruning (가지치기)**: 불필요한 분기를 제거하여 모델 복잡도를 줄입니다.
*   **내장된 교차 검증(Cross-Validation)**: 모델 성능 평가를 위한 교차 검증을 지원합니다.
*   **다양한 목적 함수(Objective Function)**: 회귀, 분류(이진, 다중), 랭킹 등 다양한 문제에 적용 가능합니다.
*   **DMatrix**: XGBoost 전용 데이터 구조로, 계산 효율성과 메모리 절감을 제공합니다.

## 2. XGBoost의 핵심 구성 요소

### 2.1 Booster

Booster는 XGBoost의 학습 방식을 결정하는 핵심 요소입니다.

*   **`gbtree`**: 결정 트리 기반 부스터. 가장 널리 사용되며, 복잡한 비선형 관계를 잘 학습합니다.
    *   `max_depth`: 트리의 최대 깊이. 깊을수록 모델이 복잡해지며, 과적합 위험이 증가합니다. (일반적으로 3~10 사이의 값)
    *   `min_child_weight`: 노드 분할에 필요한 최소 가중치 합. 높을수록 과적합을 방지하지만, 너무 높으면 과소적합(Underfitting)될 수 있습니다.
    *   `gamma`: 트리 분할을 위한 최소 손실 감소량. 값이 클수록 보수적인 모델이 됩니다.
    *   `subsample`: 각 트리 생성 시 사용할 데이터 샘플 비율 (0~1). 과적합 방지에 도움이 됩니다.
    *   `colsample_bytree`: 각 트리 생성 시 사용할 특성(feature) 비율 (0~1). 과적합 방지에 도움이 됩니다.
    *   `eta` (learning rate): 학습률. 각 단계에서 가중치를 얼마나 업데이트할지 결정합니다. 작을수록 보수적인 학습을 합니다. (일반적으로 0.01~0.3)

*   **`gblinear`**: 선형 모델 기반 부스터. 선형 관계를 가지는 데이터에 적합하며, 계산 속도가 빠릅니다.
    *   `alpha`: L1 정규화 항의 가중치. (Lasso Regression)
    *   `lambda`: L2 정규화 항의 가중치. (Ridge Regression)

*   **`dart`**:  Dropouts meet Multiple Additive Regression Trees. gbtree에 드롭아웃(Dropout)을 추가하여 과적합을 방지하는 방식입니다.
    *   `rate_drop`: 각 트리 생성 시 드롭아웃될 트리의 비율
    *    `skip_drop`: 드롭아웃을 건너뛸 확률

### 2.2 Objective (목적 함수)

목적 함수는 모델이 학습 과정에서 최소화해야 할 손실(Loss)을 정의합니다.

*   **`reg:squarederror`**: 회귀 문제. 평균 제곱 오차(Mean Squared Error)를 사용합니다.
*   **`binary:logistic`**: 이진 분류 문제. 로지스틱 손실(Logistic Loss)을 사용합니다.
*   **`multi:softmax`**: 다중 분류 문제. 소프트맥스 함수를 사용하여 각 클래스에 속할 확률을 계산합니다.
*   **`rank:pairwise`**: 랭킹 문제. Pairwise ranking loss를 사용합니다.
* **`survival:cox`**: 생존 분석 문제. Cox proportional hazards model을 사용.

### 2.3 DMatrix

DMatrix는 XGBoost 전용 데이터 구조로, 데이터를 효율적으로 저장하고 처리합니다.

*   희소 행렬(Sparse Matrix)을 지원하여 메모리 사용량을 줄입니다.
*   데이터와 레이블(label)을 함께 저장하여 학습 과정을 최적화합니다.

```python
import xgboost as xgb
import numpy as np

# 예제 데이터 (2개의 특성, 10개의 샘플)
data = np.random.rand(10, 2)
label = np.random.randint(0, 2, 10)  # 이진 분류 레이블

# DMatrix 생성
dtrain = xgb.DMatrix(data, label=label)

print(dtrain)
```

## 3. Scikit-learn Wrapper

XGBoost는 Scikit-learn API와 호환되는 Wrapper를 제공하여, Scikit-learn의 다른 모델들과 동일한 방식으로 사용할 수 있습니다.

*   **`XGBClassifier`**: 분류 모델
*   **`XGBRegressor`**: 회귀 모델

```python
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Iris 데이터셋 로드
iris = load_iris()
X, y = iris.data, iris.target

# 학습/테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost 분류 모델 생성 및 학습
xgb_clf = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, objective='multi:softmax', use_label_encoder=False)
# use_label_encoder=False 경고를 없애기 위한 옵션.  XGBoost 1.3.0 이후 버전부터는 label encoding 경고가 발생하므로 False 추가
xgb_clf.fit(X_train, y_train)

# 예측
y_pred = xgb_clf.predict(X_test)

# 정확도 평가
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# XGBoost 회귀 모델 예제
xgb_reg = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')
# ... (회귀 모델 학습 및 평가 코드) ...
```

## 4. 과적합 방지 기법

과적합은 모델이 학습 데이터에 너무 맞춰져서 새로운 데이터에 대한 일반화 성능이 떨어지는 현상입니다. XGBoost는 다음과 같은 방법으로 과적합을 방지합니다.

### 4.1 Cross-Validation (교차 검증)

데이터를 여러 개의 폴드(fold)로 나누어, 각 폴드를 번갈아 가며 검증 세트로 사용하여 모델의 성능을 평가합니다.  이를 통해 모델이 특정 데이터에 과도하게 최적화되는 것을 방지합니다.

```python
import xgboost as xgb
from sklearn.model_selection import KFold

# K-Fold 교차 검증 설정
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# XGBoost 모델 생성
xgb_model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False)

# 교차 검증 수행
results = xgb.cv(
    params=xgb_model.get_xgb_params(), # 모델의 현재 파라미터
    dtrain=dtrain,  # 훈련 데이터 (DMatrix)
    nfold=5,       # 폴드 개수
    metrics={'error'}, # 평가 지표 (여기서는 오류율)
    early_stopping_rounds=10, # 조기 종료 조건
    seed=42        # 랜덤 시드
)

print(results)  # 교차 검증 결과 출력

```

### 4.2 Early Stopping (조기 종료)

검증 데이터의 성능이 더 이상 향상되지 않으면 학습을 조기에 중단합니다. `early_stopping_rounds` 파라미터를 사용하여, 지정된 횟수만큼 성능 향상이 없으면 학습을 멈춥니다.

```python
# XGBoost 모델 학습 (조기 종료 사용)
xgb_model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],  # 검증 데이터
              early_stopping_rounds=10)

```

### 4.3 Pruning (가지치기)

Tree Pruning은 불필요한 분기(가지)를 제거하여 모델의 복잡도를 줄이는 기법입니다.

*   **Pre-pruning**: 트리 생성 과정에서 미리 조건을 설정하여 분기를 제한합니다. (`max_depth`, `min_child_weight`, `gamma` 등의 파라미터 사용)
*   **Post-pruning**: 트리를 완전히 생성한 후, 성능에 기여하지 않는 분기를 제거합니다. (XGBoost는 내부적으로 post-pruning을 수행)

### 4.4 Regularization (정규화)

정규화는 모델의 복잡도에 페널티를 부여하여 과적합을 방지하는 기법입니다.

* **L1 Regularization (alpha)**: 가중치의 절댓값 합을 제한합니다. 일부 가중치를 0으로 만들어 희소한(sparse) 모델을 만듭니다. (특성 선택 효과)
* **L2 Regularization (lambda)**: 가중치의 제곱 합을 제한합니다. 가중치를 작게 유지하여 모델을 더 부드럽게 만듭니다.
* **gamma**: 리프 노드를 추가하기 위한 최소 손실 감소량.  값이 클수록 더 보수적인 모델이 됩니다.

```python
# XGBoost 모델 (정규화 적용)
xgb_reg = XGBRegressor(n_estimators=100,
                       max_depth=3,
                       learning_rate=0.1,
                       objective='reg:squarederror',
                       reg_alpha=0.1,  # L1 정규화
                       reg_lambda=0.1) # L2 정규화
```
### 4.5 그 외 파라미터 튜닝

*   `subsample`: 각 트리 생성 시 사용할 데이터 샘플의 비율을 조절합니다. (0 < subsample ≤ 1)
*    `colsample_bytree`: 각 트리 생성 시 사용할 특성(feature)의 비율을 조절합니다. (0 < colsample_bytree ≤ 1)

이러한 파라미터들을 적절히 조절하여 모델의 복잡도를 제어하고 과적합을 방지할 수 있습니다.

## 5. GPU 가속

XGBoost는 GPU를 사용하여 학습 속도를 크게 향상시킬 수 있습니다. `tree_method` 파라미터를 `gpu_hist`로 설정하면 GPU를 사용합니다. (CUDA 지원 GPU 필요)

```python
xgb_model = XGBClassifier(tree_method='gpu_hist',
                        n_estimators=100,
                        max_depth=3,
                        learning_rate=0.1, objective='multi:softmax', use_label_encoder=False)
```

## 결론

XGBoost는 강력하고 효율적인 머신러닝 알고리즘으로, 다양한 문제에 적용할 수 있습니다. 이 글에서 설명한 개념, 원리, 과적합 방지 기법을 이해하고 적절히 활용하면 XGBoost를 사용하여 뛰어난 성능의 모델을 구축할 수 있습니다.
