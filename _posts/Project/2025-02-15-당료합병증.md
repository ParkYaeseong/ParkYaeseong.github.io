---
title: ë‹¹ë£Œí•©ë³‘ì¦ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜
date: 2025-02-13 13:28:00 +09:00
categories: [Project, ë‹¹ë£Œí•©ë³‘ì¦]
tags: [ë‹¹ë£Œí•©ë³‘ì¦, R]
---

## í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(smotefamily)  # ë¶ˆê· í˜• ë°ì´í„° í•´ê²°
library(fastDummies)  # One-hot encoding
library(pROC)  # ROC Curve
library(reshape2)
library(glmnet)  # Lasso ë° Ridge íšŒê·€

set.seed(300)
```

## âœ… 1. ë°ì´í„° ìƒì„±
```{r}
data <- data.frame(
  Characteristic = c(
    rep("Gender", 2),
    rep("Age_group", 3),
    rep("Residential_area", 3),
    rep("Income", 3),
    rep("DM_duration", 3),
    rep("BMI_group", 4),
    rep("SBP_group", 2),
    rep("DBP_group", 2),
    rep("FBS_group", 2),
    rep("Cholesterol_group", 2),
    rep("Proteinuria", 2),
    rep("Smoking", 3),
    rep("Alcohol", 3),
    rep("Physical_activity", 5),
    rep("Medication", 2)
  ),
  
  Category = c(
    "Men", "Women",
    "30â€“49", "50â€“69", "70â€“89",
    "Capital", "Metropolitan", "Non-metropolitan",
    "1â€“3", "4â€“6", "7â€“10",
    "4~<7", "7~<10", "â‰¥10",
    "<18.5", "18.5~22.9", "23.0~24.9", "â‰¥25",
    "<130", "â‰¥130",
    "<85", "â‰¥85",
    "<100", "â‰¥100",
    "<220", "â‰¥220",
    "Negative", "Positive",
    "Never", "Quit", "Current",
    "<1/month", "1-4/month", "â‰¥Twice/week",
    "None", "1-2", "3-4", "5-6", "Everyday",
    "<80", "â‰¥80"
  ),
  
  Complication_No = c(
    180, 35,
    99, 108, 8,
    29, 77, 109,
    45, 38, 132,
    6, 76, 133,
    1, 63, 51, 100,
    95, 120,
    140, 75,
    39, 176,
    145, 70,
    204, 11,
    111, 24, 80,
    81, 99, 35,
    89, 70, 32, 6, 18,
    53, 162
  ),
  
  Complication_Yes = c(
    886, 306,
    365, 739, 88,
    204, 322, 666,
    271, 239, 682,
    26, 351, 815,
    10, 267, 319, 596,
    470, 722,
    721, 471,
    235, 957,
    800, 392,
    1125, 67,
    712, 124, 356,
    557, 483, 152,
    521, 353, 179, 47, 92,
    334, 858
  )
)
```

## âœ… 2. ë°ì´í„° ë³€í™˜ (long format)
```{r}
data_long <- data %>%
  pivot_longer(
    cols = c(Complication_No, Complication_Yes),
    names_to = "Complication",
    values_to = "Count"
  ) %>%
  uncount(Count) %>%
  mutate(
    Complication = ifelse(Complication == "Complication_Yes", 1, 0),  # 1: Yes, 0: No
    Complication = as.factor(Complication),
    Characteristic = as.factor(Characteristic),
    Category = as.factor(Category)
  ) %>%
  select(-Characteristic)  # Characteristic ì»¬ëŸ¼ ì œê±°
```

## âœ… 3. Train/Test ë°ì´í„° ë¶„í•  (70:30)
```{r}
set.seed(300)
train_index <- createDataPartition(data_long$Complication, p = 0.7, list = FALSE)
train_data <- data_long[train_index, ]
test_data <- data_long[-train_index, ]
```

## âœ… 4. One-hot Encoding (ëª¨ë“  ë²”ì£¼í˜• ë³€ìˆ˜ ë³€í™˜)
```{r}
train_encoded <- dummy_cols(train_data, select_columns = "Category", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
test_encoded <- dummy_cols(test_data, select_columns = "Category", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
```

## âœ… 5. ë³€ìˆ˜ëª… ë³€í™˜ (íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬)
```{r}
fix_column_names <- function(df) {
  colnames(df) <- gsub("<", "Less_", colnames(df))
  colnames(df) <- gsub(">", "More_", colnames(df))
  colnames(df) <- gsub("~", "Range_", colnames(df))
  colnames(df) <- gsub("-", "_", colnames(df))
  colnames(df) <- make.names(colnames(df), unique = TRUE)
  return(df)
}

train_encoded <- fix_column_names(train_encoded)
test_encoded <- fix_column_names(test_encoded)
```

## âœ… 6. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (SMOTE ì´ì „ ìˆ˜í–‰)
```{r}
preProcess_model <- preProcess(train_encoded, method = c("center", "scale"))
train_scaled <- predict(preProcess_model, train_encoded)
test_scaled <- predict(preProcess_model, test_encoded)
```

## âœ… 7. SMOTE ì ìš© (ìŠ¤ì¼€ì¼ë§ëœ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)
```{r}
smote_train <- SMOTE(train_scaled[,-1], train_scaled$Complication)
train_balanced <- smote_train$data %>%
  rename(Complication = class) %>%
  mutate(Complication = as.factor(Complication))

  test_balanced <- test_scaled
```

 ğŸ“Œ SMOTE ì ìš© ì „ í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
table(data_long$Complication)
table(train_scaled$Complication)  # ì›ë³¸ ë°ì´í„° ë¹„ìœ¨

 ğŸ“Œ SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
table(train_balanced$Complication)  # SMOTE ì ìš© í›„ í™•ì¸
table(test_balanced$Complication)




## âœ… 8. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
```{r}
logit_model <- glm(Complication ~ ., family = binomial, data = train_balanced)
summary(logit_model)
```

## âœ… 9. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
```{r}
rf_model <- randomForest(Complication ~ ., data = train_balanced, ntree = 500, importance = TRUE)
print(rf_model)
```

## âœ… 10. ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
```{r}
evaluate_model <- function(model, test_data) {
  # ì˜ˆì¸¡ê°’ ê³„ì‚° (ë¡œì§€ìŠ¤í‹± íšŒê·€ vs ëœë¤ í¬ë ˆìŠ¤íŠ¸)
  if ("glm" %in% class(model)) {
    pred_prob <- predict(model, newdata = test_data, type = "response")  # í™•ë¥  ì˜ˆì¸¡
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)  # 0 ë˜ëŠ” 1ë¡œ ë³€í™˜
  } else if ("randomForest" %in% class(model)) {
    pred_class <- predict(model, newdata = test_data, type = "class")  # ì´ë¯¸ 0 ë˜ëŠ” 1 í˜•íƒœ
  }
  
  # factor í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í´ë˜ìŠ¤ ì¼ì¹˜)
  pred_class <- as.factor(pred_class)
  actual_class <- as.factor(test_data$Complication)
  
  # Confusion Matrix ê³„ì‚°
  cm <- confusionMatrix(pred_class, actual_class)
  print(cm)
}
```

## âœ… 11. ëª¨ë¸ í‰ê°€ ì‹¤í–‰
```{r}
evaluate_model(logit_model, test_scaled)
evaluate_model(rf_model, test_scaled)
```

## âœ… 12. ROC Curve ë° AUC ê³„ì‚°
```{r}
pred_prob <- predict(logit_model, newdata = test_scaled, type = "response")
roc_curve <- roc(test_scaled$Complication, pred_prob)
plot(roc_curve, col = "blue", main = "ROC Curve (Logistic Regression)")
print(auc(roc_curve))

rf_pred_prob <- predict(rf_model, newdata = test_scaled, type = "prob")[,2]
rf_roc_curve <- roc(test_scaled$Complication, rf_pred_prob)
plot(rf_roc_curve, col = "red", main = "ROC Curve (Random Forest)")
print(auc(rf_roc_curve))
```

## âœ… 13. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë³€ìˆ˜ ì¤‘ìš”ë„ í™•ì¸
```{r}
importance_vals <- importance(rf_model) %>%
  as.data.frame() %>%
  rownames_to_column("Feature") %>%
  arrange(desc(MeanDecreaseGini))

# ìƒìœ„ 20ê°œ ë³€ìˆ˜ ì‹œê°í™”
ggplot(head(importance_vals, 20), aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue", width = 0.7) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 20 Important Features (Random Forest)",
       x = "Feature",
       y = "Mean Decrease in Gini")

# ì¤‘ìš”í•œ ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê¸°

logit_summary <- summary(logit_model)
p_values <- logit_summary$coefficients[, 4]
# p-valueê°€ 0.05 ë¯¸ë§Œì¸ ë³€ìˆ˜ ì„ íƒ
significant_vars <- names(which(logit_summary$coefficients[, 4] < 0.05))

# Intercept(ì ˆí¸) ì œê±°
significant_vars <- significant_vars[!significant_vars %in% "(Intercept)"]

# ì„ íƒëœ ë³€ìˆ˜ ì¶œë ¥
print(significant_vars)


rf_importance <- importance(rf_model) %>%
  as.data.frame() %>%
  rownames_to_column("Feature") %>%
  arrange(desc(MeanDecreaseGini))  # ì¤‘ìš”ë„ ë†’ì€ ìˆœ ì •ë ¬

# ìƒìœ„ 10ê°œ ë³€ìˆ˜ í™•ì¸
top_rf_vars <- head(rf_importance$Feature, 10)
print(top_rf_vars)

# ë¡œì§€ìŠ¤í‹± íšŒê·€ & ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê³µí†µ ë³€ìˆ˜ ì°¾ê¸°
selected_vars <- intersect(significant_vars, top_rf_vars)

print("ìµœì¢… ì„ íƒëœ ë³€ìˆ˜ ëª©ë¡:")
print(selected_vars)

# ì„ íƒëœ ë³€ìˆ˜ë§Œ í¬í•¨í•œ ë°ì´í„°ì…‹ ìƒì„±
train_selected <- train_balanced %>% select(all_of(c("Complication", selected_vars)))
test_selected <- test_scaled %>% select(all_of(c("Complication", selected_vars)))

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ì¬í•™ìŠµ
logit_model_selected <- glm(Complication ~ ., family = binomial, data = train_selected)
summary(logit_model_selected)

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì¬í•™ìŠµ
rf_model_selected <- randomForest(Complication ~ ., 
                                  data = train_selected, 
                                  ntree = 500, 
                                  importance = TRUE)


evaluate_model(logit_model_selected, test_scaled)
evaluate_model(rf_model_selected, test_scaled)

# ë¡œì§€ìŠ¤í‹± íšŒê·€ ROC Curve
pred_prob_selected <- predict(logit_model_selected, newdata = test_selected, type = "response")
roc_curve_selected <- roc(test_selected$Complication, pred_prob_selected)
plot(roc_curve_selected, col = "blue", main = "ROC Curve (Selected Features - Logistic Regression)")
print(auc(roc_curve_selected))

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ROC Curve
rf_pred_prob_selected <- predict(rf_model_selected, newdata = test_selected, type = "prob")[,2]
rf_roc_curve_selected <- roc(test_selected$Complication, rf_pred_prob_selected)
plot(rf_roc_curve_selected, col = "red", main = "ROC Curve (Selected Features - Random Forest)")
print(auc(rf_roc_curve_selected))
```

# âœ… 14. Lasso ë° Ridge íšŒê·€ ì ìš©
```{r}
x <- model.matrix(Complication ~ ., train_balanced)[,-1]
y <- as.numeric(train_balanced$Complication) - 1  # 0ê³¼ 1 ë³€í™˜

# Lasso (L1 ì •ê·œí™”)
cv_lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")  # alpha=1 â†’ Lasso
lasso_model <- glmnet(x, y, alpha = 1, lambda = cv_lasso$lambda.min)

# Ridge (L2 ì •ê·œí™”)
cv_ridge <- cv.glmnet(x, y, alpha = 0, family = "binomial")  # alpha=0 â†’ Ridge
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_ridge$lambda.min)

print(cv_lasso$lambda.min)  # ìµœì ì˜ ë¼ì˜ ì •ê·œí™” íŒŒë¼ë¯¸í„°
print(cv_ridge$lambda.min)  # ìµœì ì˜ ë¦¿ì§€ ì •ê·œí™” íŒŒë¼ë¯¸í„°


#  Lasso ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
lasso_model <- glmnet(x, y, alpha = 1, lambda = cv_lasso$lambda.min)

# Lasso ëª¨ë¸ ì˜ˆì¸¡
lasso_pred_prob <- predict(lasso_model, newx = model.matrix(Complication ~ ., test_scaled)[,-1], type = "response")
lasso_pred_class <- ifelse(lasso_pred_prob > 0.5, 1, 0)

# Lasso ëª¨ë¸ í‰ê°€
lasso_cm <- confusionMatrix(as.factor(lasso_pred_class), as.factor(test_scaled$Complication))
print(lasso_cm)

# Lasso ROC Curve ë° AUC
lasso_roc <- roc(test_scaled$Complication, lasso_pred_prob)
plot(lasso_roc, col = "blue", main = "ROC Curve (Lasso)")
print(auc(lasso_roc))
```

### âœ… 2Ridge ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```{r}
ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_ridge$lambda.min)

# Ridge ëª¨ë¸ ì˜ˆì¸¡
ridge_pred_prob <- predict(ridge_model, newx = model.matrix(Complication ~ ., test_scaled)[,-1], type = "response")
ridge_pred_class <- ifelse(ridge_pred_prob > 0.5, 1, 0)

# Ridge ëª¨ë¸ í‰ê°€
ridge_cm <- confusionMatrix(as.factor(ridge_pred_class), as.factor(test_scaled$Complication))
print(ridge_cm)

# Ridge ROC Curve ë° AUC
ridge_roc <- roc(test_scaled$Complication, ridge_pred_prob)
plot(ridge_roc, col = "red", main = "ROC Curve (Ridge)")
print(auc(ridge_roc))
```


## âœ… 15. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ ì¶”ê°€
```{r}
library(rpart)
library(rpart.plot)

# ê¸°ë³¸ ì˜ì‚¬ê²°ì •ë‚˜ë¬´
tree_model <- rpart(Complication ~ ., 
                    data = train_balanced,
                    method = "class",
                    control = rpart.control(cp = 0.001, minsplit = 20))

# ê°€ì§€ì¹˜ê¸° ìˆ˜í–‰ (ìµœì  cp ê°’ ì°¾ê¸°)
plotcp(tree_model)
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]
pruned_tree <- prune(tree_model, cp = best_cp)

# ëª¨ë¸ ì‹œê°í™”
rpart.plot(pruned_tree, 
           extra = 104, 
           box.palette = "GnBu",
           shadow.col = "gray",
           nn = TRUE)

# ì„±ëŠ¥ í‰ê°€
tree_pred <- predict(pruned_tree, test_scaled, type = "class")
tree_prob <- predict(pruned_tree, test_scaled, type = "prob")[,2]

# í˜¼ë™ í–‰ë ¬
print(confusionMatrix(tree_pred, test_scaled$Complication))

# ROC ê³¡ì„ 
tree_roc <- roc(test_scaled$Complication, tree_prob)
plot(tree_roc, col="green", main="ROC Curve (Decision Tree)")
auc(tree_roc)
```

# âœ… 16. ë‹¤ì¤‘ê³µì„ ì„±
```{r}
library(car)

# ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸ (VIF ê³„ì‚°)
logit_model <- glm(Complication ~ ., family = binomial, data = train_balanced)
vif_values <- car::vif(logit_model)
print(vif_values)

# VIF ì‹œê°í™”
barplot(vif_values, col = "steelblue", horiz = TRUE, las = 1)
abline(v = 2, col = "red", lty = 2)

# VIF > 2 ì¸ ë³€ìˆ˜ ì œê±° (ë³´í†µ 5~10ì¸ë° ì°¨ì´ê°€ ì—†ì–´ì„œ)
high_vif <- names(which(vif_values > 2))
train_reduced <- train_balanced %>% select(-all_of(high_vif))
test_reduced <- test_balanced %>% select(-all_of(high_vif))

# ë°ì´í„° ì •ê·œí™” (í‘œì¤€í™”)
preProcess_model <- preProcess(train_reduced, method = c("center", "scale"))
train_normalized <- predict(preProcess_model, train_reduced)
test_normalized <- predict(preProcess_model, test_reduced)

# ëª¨ë¸ ì¬í•™ìŠµ
logit_reduced <- glm(Complication ~ ., family = binomial, data = train_normalized)
evaluate_model(logit_reduced, test_balanced)


# âœ… íšŒê·€ ëª¨ë¸ í‰ê°€ (AUC í¬í•¨)
pred_prob <- predict(logit_reduced, newdata = test_balanced, type = "response")

# âœ… ROC Curve ë° AUC ê³„ì‚°
roc_curve <- roc(test_balanced$Complication, pred_prob)
auc_value <- auc(roc_curve)

# âœ… ê²°ê³¼ ì¶œë ¥
print(auc_value)  # AUC ê°’ ì¶œë ¥
```

## ê²°ë¡  
: í˜„ì¬ ëª¨ë¸ë“¤ì€ ì‹¤ì œ ì„ìƒì  ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ ëŠ¥ë ¥ì´ ì—†ìœ¼ë©°, ë°ì´í„°ì˜ ê·¼ë³¸ì  í•œê³„(ì˜ˆì¸¡ë³€ìˆ˜-ê²°ê³¼ ê°„ ì—°ê´€ì„± ë¶€ì¡±)ê°€ ì£¼ëœ ì›ì¸ì…ë‹ˆë‹¤. 
ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‹œê°í™” ê²°ê³¼ì—ì„œë„ ë³µì¡í•œ ê·œì¹™ ì—†ì´ ë¬´ì‘ìœ„ ë¶„í• ì´ ê´€ì°°ë©ë‹ˆë‹¤. 
í•´ê²°ì„ ìœ„í•´ì„œëŠ” ë” ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ë³€ìˆ˜ ìˆ˜ì§‘ê³¼ ì „ë¬¸ê°€ í”¼ë“œë°±ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
