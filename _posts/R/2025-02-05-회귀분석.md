---
title: 회귀분석
date: 2025-02-05 01:00:00 +09:00
categories: [R, 데이터]
tags: [R, R Studion, 회귀분석석]
---
<!-- _includes/head.html -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


---

## 1. 회귀 분석(Regression Analysis)
### ▎ 기본 개념
- **목적**: 독립 변수(X)와 종속 변수(Y) 간의 관계를 모델링  
- **방정식**:  
  $$ Y_i = \beta_0 + \beta_1 X_i + \epsilon_i $$  
  $\beta_0$: 절편, $\beta_1$: 기울기, $\epsilon_i$: 오차항  

### ▎ 가정
1. **선형성**: X와 Y의 관계가 선형  
2. **등분산성**: 오차항의 분산이 일정  
3. **정규성**: 오차항이 정규분포를 따름  
4. **독립성**: 오차항 간 상관관계 없음  

---

## 2. 최소제곱법(Ordinary Least Squares, OLS)
### ▎ 원리
- **목표**: 잔차 제곱합(SSE)을 최소화하는 계수($\beta_0$, $\beta_1$) 추정  
- **계산식**:  
  $$ \min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$  
  $\hat{y}_i = \beta_0 + \beta_1 X_i$: 예측값  

### ▎ 계수 추정
- **기울기 ($\beta_1$)**:  
  $$ \beta_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} $$  
- **절편 ($\beta_0$)**:  
  $$ \beta_0 = \bar{Y} - \beta_1 \bar{X} $$  

---

## 3. 제곱합(SSR, SSE, SST)과 결정계수
### ▎ 제곱합 관계

| 개념              | 수식                  | 설명                          |
|--------------------|-----------------------|-------------------------------|
| **SST** (전체 제곱합) | $\sum (y_i - \bar{y})^2$ | 종속 변수의 전체 변동성 <br> 실제 관측값과 평균값 간의 차이를 제곱하여 합한 값      |
| **SSR** (회귀 제곱합) | $\sum (\hat{y}_i - \bar{y})^2$ | 모델이 설명하는 변동성 <br> 종속 변수의 예측값과 평균값 간의 차이를 제곱하여 합한 값         |
| **SSE** (잔차 제곱합) | $\sum (y_i - \hat{y}_i)^2$ | 모델이 설명하지 못하는 변동성 <br> 제 관측값과 예측값 간의 차이를 제곱하여 합한 값   |

- **관계식**:  
  $$ \text{SST} = \text{SSR} + \text{SSE} $$  

### ▎ 결정계수(R²)
- **정의**: 모델이 설명하는 변동성의 비율  
- **계산식**:  
  $$ R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}} $$  
- **특징**:  
  - `0 ≤ R² ≤ 1` (높을수록 모델 성능 좋음)  
  - **과적합 주의**: 변수 추가 시 항상 증가함  

---

## 4. AIC(Akaike Information Criterion)
### ▎ 개념
- 변수가 커짐에 따라 Bias(편차)가 줄어들고 분산(variance)는 커짐
- **목적**: 모델의 **적합도**와 **복잡도**를 동시에 고려한 평가  
- **공식**:  
  $$ AIC = n \ln\left(\frac{\text{SSE}}{n}\right) + 2K $$  
  - $n$: 표본 크기, $K$: 모델의 파라미터 수  
  - **SSE 사용**: 잔차 제곱합이 작을수록 좋음  
  - **페널티 항($2K$)**: 변수 증가 시 패널티 부과  

### ▎ 해석
- **낮은 AIC** → 더 우수한 모델  
- **특징**:  
  - 과적합 방지: 불필요한 변수 추가를 억제  
  - 모델 비교: 다른 모델 간 AIC 차이로 성능 평가  

---

## 5. Bias-Variance Tradeoff
### ▎ 모델 복잡도에 따른 변화

|                     | **편향(Bias)**       | **분산(Variance)**    |
|---------------------|----------------------|-----------------------|
| **단순 모델** (적은 변수) | 높음 (과소적합)      | 낮음                  |
| **복잡 모델** (많은 변수) | 낮음                 | 높음 (과적합)         |

### ▎ AIC의 역할
- **최적점 탐색**: 편향과 분산이 균형을 이루는 지점 선택  
- **일반화 성능**: 테스트 데이터에서 좋은 예측력을 보장  

---

## 6. 요약: 회귀 분석 Workflow
1. **모델 구축**: 최소제곱법으로 계수 추정  
2. **성능 평가**: R²로 설명력 확인  
3. **모델 선택**: AIC로 복잡도와 적합도 균형 검토  
4. **진단**: 잔차 분석을 통해 가정 검증  

---

> **키 포인트**  
> - **최소제곱법**: 잔차 최소화를 통한 최적 계수 추정  
> - **AIC**: "Less is more" – 불필요한 변수 추가를 방지  
> - **R² vs AIC**: R²는 설명력, AIC는 일반화 성능에 초점  