---
title: 딥러닝 기초(2)
date: 2025-03-25 05:11:00 +09:00
categories: [딥러닝]
tags: [딥러닝, TensorFlow, Keras]
---

## 딥러닝 모델 개발 완전 가이드: TensorFlow와 Keras를 활용한 실전 예제

###   **1. 딥러닝 모델 개발 환경 설정**

   딥러닝 모델을 개발하기 위해서는 TensorFlow와 Keras 라이브러리가 필요합니다. Google Colab과 같은 환경에서는 이미 설치되어 있을 수 있지만, 로컬 환경에서는 별도로 설치해야 합니다.


   pip install tensorflow


###   **2. Keras를 이용한 모델 구성 방법**

   Keras는 신경망 모델을 쉽게 구성할 수 있도록 도와주는 고수준 API입니다. Keras에서 모델을 구성하는 방법은 크게 3가지가 있습니다.

   * **Sequential Model**: 레이어를 선형으로 연결하여 간단한 모델을 구성할 때 사용합니다.

     from tensorflow.keras.models import Sequential
     from tensorflow.keras.layers import Dense, Input

     model = Sequential()
     model.add(Input(shape=(8,)))  # 입력 레이어
     model.add(Dense(12, activation='relu'))  # 첫 번째 Dense 레이어
     model.add(Dense(8, activation='relu'))  # 두 번째 Dense 레이어
     model.add(Dense(1, activation='sigmoid'))  # 출력 레이어


   * **Functional API**: 더 복잡하고 유연한 모델을 구성할 때 사용합니다. 다중 입력, 다중 출력, 레이어 공유 등이 가능합니다.


     from tensorflow.keras import layers, Model

     inputs = keras.Input(shape=(784,), name='digits')
     x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
     x = layers.Dense(64, activation='relu', name='dense_2')(x)
     outputs = layers.Dense(10, activation='softmax', name='predictions')(x)

     model = keras.Model(inputs=inputs, outputs=outputs)


   * **Model 상속**: 사용자 정의 레이어를 만들거나 모델의 동작을 세밀하게 제어해야 할 때 사용합니다.

###   **3. 주요 개념 설명**

   * **가중치 (Weight)**: 입력 데이터의 중요도를 조절하는 역할을 합니다. 가중치가 클수록 해당 입력값이 모델 출력에 미치는 영향이 커집니다.
   * **활성화 함수 (Activation Function)**: 노드의 출력값을 결정하는 함수입니다. 모델에 비선형성을 추가하여 복잡한 패턴을 학습할 수 있도록 합니다.
     * **ReLU (Rectified Linear Unit)**: 0보다 큰 값은 그대로 출력하고, 0보다 작은 값은 0으로 만듭니다.
     * **Sigmoid**: 출력값을 0과 1 사이로 제한합니다. 주로 이진 분류 문제에서 사용됩니다.
   * **손실 함수 (Loss Function)**: 모델의 예측값과 실제값의 차이를 측정하는 함수입니다.
     * **Binary Crossentropy**: 이진 분류 문제에서 사용됩니다.
     * **Mean Squared Error (MSE)**: 회귀 문제에서 사용됩니다.
   * **최적화 알고리즘 (Optimizer)**: 손실 함수를 최소화하는 방향으로 모델의 가중치를 업데이트하는 알고리즘입니다.
     * **Adam (Adaptive Moment Estimation)**: 학습률을 적응적으로 조절하며, 모멘텀을 활용하여 학습 속도를 높입니다.
   * **검증 (Validation)**: 모델의 성능을 평가하고 과적합을 방지하기 위해 학습 데이터의 일부를 검증 데이터로 사용하는 과정입니다.
   * **과적합 (Overfitting)**: 모델이 학습 데이터에 너무 잘 맞추어져서 새로운 데이터에 대한 예측 성능이 떨어지는 현상입니다.

###   **4. 딥러닝 모델 개발 예제**

   ####   **4.1 이진 분류 모델 개발**


   import numpy
   import pandas as pd
   import tensorflow as tf
   from tensorflow.keras.models import Sequential
   from tensorflow.keras.layers import Dense, Input
   from sklearn.preprocessing import LabelEncoder
   from sklearn.model_selection import train_test_split
   import matplotlib.pyplot as plt

   # 데이터 로드
   df = pd.read_csv("/content/sonar.csv", header=None)
   dataset = df.values

   X = dataset[:, 0:60].astype(numpy.float32)  # 입력 데이터
   Y_obj = dataset[:, 60]  # 출력 데이터

   # 레이블 인코딩
   e = LabelEncoder()
   e.fit(Y_obj)
   Y = e.transform(Y_obj)

   # 학습 데이터와 테스트 데이터 분리
   X_train, X_test, Y_train, Y_test = train_test_split(
       X, Y, test_size=0.3, random_state=0
   )

   # 모델 구성
   model = Sequential()
   model.add(Input(shape=(60,)))
   model.add(Dense(24, activation='relu'))
   model.add(Dense(10, activation='relu'))
   model.add(Dense(1, activation='sigmoid'))

   # 모델 컴파일
   model.compile(
       loss='mean_squared_error', optimizer='adam', metrics=['accuracy']
   )

   # 모델 학습
   history = model.fit(
       X_train, Y_train, epochs=130, batch_size=5, validation_data=(X_test, Y_test)
   )

   # 학습 결과 시각화
   training_loss = history.history['loss']
   val_loss = history.history['val_loss']
   epoch_count = range(1, len(training_loss) + 1)
   plt.plot(epoch_count, training_loss, 'r--')
   plt.plot(val_loss, label='Validation Loss')
   plt.legend(['Training Loss', 'Val Loss'])
   plt.xlabel('Epoch')
   plt.ylabel('Loss')
   plt.show()

   training_accuracy = history.history['accuracy']
   validation_accuracy = history.history['val_accuracy']

   plt.plot(epoch_count, training_accuracy, 'r--')
   plt.plot(epoch_count, validation_accuracy, 'b-')
   plt.legend(['training_accuracy', 'validation_accuracy'])
   plt.title('Model accuracy')
   plt.xlabel('Epoch')
   plt.ylabel('Loss')
   plt.show()

   # 모델 평가
   model.evaluate(X_test, Y_test)

   # 모델 저장 및 로드
   model.save('my_model.keras')
   model = tf.keras.models.load_model('my_model.keras')


   ####   **4.2 다중 분류 모델 개발**


   from tensorflow import keras
   from tensorflow.keras import layers
   import tensorflow as tf
   import numpy as np
   import matplotlib.pylab as plt
   from tensorflow.keras.utils import to_categorical

   # MNIST 데이터셋 로드
   (X_train0, y_train0), (X_test0, y_test0) = keras.datasets.mnist.load_data()

   # 데이터 전처리
   X_train = X_train0.reshape(60000, 784).astype('float32') / 255.0
   X_test = X_test0.reshape(10000, 784).astype('float32') / 255.0
   Y_train = to_categorical(y_train0, 10)
   Y_test = to_categorical(y_test0, 10)

   # 모델 구성 (Functional API)
   inputs = keras.Input(shape=(784,), name='digits')
   x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
   x = layers.Dense(64, activation='relu', name='dense_2')(x)
   outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
   model = keras.Model(inputs=inputs, outputs=outputs)

   # 모델 컴파일
   optimizer = keras.optimizers.SGD(learning_rate=1e-3)
   loss_fn = keras.losses.SparseCategoricalCrossentropy()
   model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

   # 모델 학습 (수동 경사 하강법)
   batch_size = 64
   train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train0))  # y_train0 사용
   train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

   for epoch in range(3):
       print('Start of epoch %d' % (epoch,))
       for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
           with tf.GradientTape() as tape:
               logits = model(x_batch_train)
               loss_value = loss_fn(y_batch_train, logits)  # y_batch_train 사용
           grads = tape.gradient(loss_value, model.trainable_weights)
           optimizer.apply_gradients(zip(grads, model.trainable_weights))
           if step % 200 == 0:
               print('Training loss (for one batch) at step %s: %s'
                     % (step, float(loss_value)))
               print('Seen so far: %s samples' % ((step + 1) * 64))

   # 모델 구성 (Sequential API)
   model = keras.Sequential(
       [
           layers.Input(shape=(3,)),
           layers.Dense(2, activation='relu', name='layer1'),
           layers.Dense(3, activation='relu', name='layer2'),
           layers.Dense(4, name='layer3'),
       ]
   )

   x = tf.ones((3, 3))
   y = model(x)

   model = tf.keras.Sequential()
   model.add(layers.Dense(2, activation="relu"))
   model.add(layers.Dense(3, activation="relu"))
   model.add(layers.Dense(4))

   x = tf.ones((3, 3))
   y = model(x)

   model.summary()

   # 모델 학습 및 평가
   np.random.seed(0)
   model = Sequential()
   model.add(Input(shape=(784,)))
   model.add(Dense(15, activation="sigmoid"))
   model.add(Dense(10, activation="sigmoid"))

   model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.2),
                 loss='mean_squared_error', metrics=['accuracy'])
   hist = model.fit(X_train, Y_train, epochs=30, batch_size=100,
                    validation_data=(X_test, Y_test), verbose=2)

   plt.plot(hist.history['accuracy'])
   plt.show()

   model.evaluate(X_train, Y_train, verbose=2)
   model.predict(X_test[:1, :])

   predictions = model.predict(X_test)
   class_labels = (predictions > 0.5).astype("int32")

   # TensorBoard를 이용한 시각화
   import datetime

   def create_model():
       return Sequential([
           tf.keras.layers.Flatten(input_shape=(28, 28)),
           layers.Dense(512, activation="relu"),
           tf.keras.layers.Dropout(0.2),
           Dense(10, activation="softmax")
       ])

   model = create_model()
   model.compile(optimizer='adam',
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])
   log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
   tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,
                                                        histogram_freq=1)

   hist = model.fit(x=x_train,
                    y=y_train,
                    epochs=5,
                    validation_data=(x_test, y_test),
                    callbacks=[tensorboard_callback])

   !pip install tensorboard
   !pip install ipython

   # %load_ext tensorboard
   # %tensorboard --logdir logs/fit


   ####   **4.3 Keras Regressor를 이용한 회귀 모델 개발**

   from scikeras.wrappers import KerasRegressor
   from sklearn.model_selection import cross_val_score
   from sklearn.model_selection import KFold
   from sklearn.preprocessing import StandardScaler
   from sklearn.pipeline import Pipeline
   from sklearn.model_selection import GridSearchCV

   # 데이터 로드
   dataframe = pd.read_csv('/content/Wine (1).csv', sep=',', header=None, skiprows=2)
   dataset = dataframe.values

   X = dataset[:, :-1]  # 입력 데이터
   Y = dataset[:, -1]  # 출력 데이터

   # 모델 정의 함수
   def baseline_model(optimizer='adam'):
       model = Sequential()
       model.add(Input(shape=(X.shape[1],)))
       model.add(Dense(20, kernel_initializer='normal', activation='relu'))
       model.add(Dense(6, kernel_initializer='normal', activation='relu'))
       model.add(Dense(1, kernel_initializer='normal'))
       model.compile(loss='mean_squared_error', optimizer=optimizer)
       return model

   seed = 7
   numpy.random.seed(seed)

   # KerasRegressor 래퍼 생성
   estimator = KerasRegressor(model=baseline_model, nb_epoch=100,
                              batch_size=5, verbose=0)

   # 파이프라인 생성
   estimators = []
   estimators.append(('standardize', StandardScaler()))
   estimators.append(('mlp', KerasRegressor(model=baseline_model, epochs=50,
                                            batch_size=5, verbose=2)))
   pipeline = Pipeline(estimators)
   kfold = KFold(n_splits=10)

   # Grid Search를 이용한 하이퍼파라미터 튜닝
   optimizer = ['SGD', 'Adam']
   param_grid = dict(mlp__optimizer=optimizer)
   grid = GridSearchCV(estimator=pipeline, param_grid=param_grid,
                       n_jobs=-1)
   grid_result = grid.fit(X, Y)

   print("최적의 최적화기 : %f using %s " %
         (grid_result.best_score_, grid_result.best_params_))


###   **5. 결론**

   이 튜토리얼에서는 TensorFlow와 Keras를 이용하여 딥러닝 모델을 개발하는 과정을 자세하게 설명했습니다. 예제 코드를 통해 이진 분류, 다중 분류, 회귀 모델을 개발하는 방법을 익히고, 주요 개념과 기술들을 이해하는 데 도움이 될 것입니다. 딥러닝 모델 개발은 끊임없는 실험과 학습이 필요한 분야입니다. 이 튜토리얼을 시작점으로 삼아, 다양한 데이터셋과 모델 구조를 경험해 보면서 딥러닝 개발 능력을 향상시켜 나가시길 바랍니다.